{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CATC5xzjhq1l",
        "outputId": "d8c3b63f-e760-406e-dc64-6c2c9de3baa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
          ]
        }
      ],
      "source": [
        "# convert text to lowercase\n",
        "\n",
        "# Text lowercase function\n",
        "def lowercase_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Test the function\n",
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "print(lowercase_text(input_str))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove numbers\n",
        "\n",
        "import re\n",
        "\n",
        "# Function to remove numbers\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Test the function\n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "print(remove_numbers(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkv2uKnTiA1B",
        "outputId": "35209dad-7d67-4026-9516-be02977957df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  balls in this bag, and  in the other one.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numbers to text\n",
        "\n",
        "import inflect\n",
        "\n",
        "# Initialize inflect engine\n",
        "q = inflect.engine()\n",
        "\n",
        "# Function to convert numbers to text\n",
        "def convert_number(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([q.number_to_words(word) if word.isdigit() else word for word in words])\n",
        "\n",
        "# Test the function\n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "print(convert_number(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBVtbmKviIN9",
        "outputId": "71a8df9d-5669-427d-af6d-f47d368ecae0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are three balls in this bag, and twelve in the other one.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation\n",
        "\n",
        "import string\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Test the function\n",
        "input_str = \"Hey, are you excited? :)\"\n",
        "print(remove_punctuation(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhJoUITIiRik",
        "outputId": "61f82e5d-e278-4f11-856a-11d05cbac5c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey are you excited \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = word_tokenize(text)\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Test the function\n",
        "ex_text = \"This is an example showing off stop word filtration.\"\n",
        "print(remove_stopwords(ex_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xN46x2CiZLo",
        "outputId": "a7b3b6bb-78a5-43c8-e281-f15de033d01b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to apply stemming\n",
        "def stem_words(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Test the function\n",
        "text = \"Data science uses scientific methods, algorithms, and many types of processes.\"\n",
        "print(stem_words(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpEV1uQzii1Q",
        "outputId": "58f69468-08be-43ea-ebcc-344510c8e784"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data', 'scienc', 'use', 'scientif', 'method', ',', 'algorithm', ',', 'and', 'mani', 'type', 'of', 'process', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmtization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download WordNet if not already downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to apply lemmatization\n",
        "def lemmatize_words(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
        "\n",
        "# Test the function\n",
        "text = \"Data science uses scientific methods, algorithms, and many types of processes.\"\n",
        "print(lemmatize_words(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIXWs7VniniK",
        "outputId": "2c2bdb66-2da1-4cb7-9df5-670d2613d182"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Data', 'science', 'use', 'scientific', 'methods', ',', 'algorithms', ',', 'and', 'many', 'type', 'of', 'process', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS tagging\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download POS tagger if not already downloaded\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to perform POS tagging\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "# Test the function\n",
        "text = \"You just gave me a pen.\"\n",
        "print(pos_tagging(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7bQyKCwkRTo",
        "outputId": "1b6d04cb-500b-4e5a-ccf8-d0ba2f519b04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('You', 'PRP'), ('just', 'RB'), ('gave', 'VBD'), ('me', 'PRP'), ('a', 'DT'), ('pen', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to perform chunking\n",
        "def chunking(text, grammar):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    parser = nltk.RegexpParser(grammar)\n",
        "    tree = parser.parse(tagged_tokens)\n",
        "    for subtree in tree.subtrees():\n",
        "        print(subtree)\n",
        "\n",
        "# Define a simple grammar for noun phrases (NP)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# Test the function\n",
        "text = \"The little red parrot is flying in the sky.\"\n",
        "chunking(text, grammar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFOh2kkmkbcN",
        "outputId": "15979e60-00d7-488f-c213-6303d7fc477c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN)\n",
            "  ./.)\n",
            "(NP The/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# named entity recognition\n",
        "\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "\n",
        "# Download NER packages if not already downloaded\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Function for Named Entity Recognition\n",
        "def ner(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    return ne_chunk(tagged_tokens)\n",
        "\n",
        "# Test the function\n",
        "text = \"Barack Obama was the 44th President of the United States.\"\n",
        "print(ner(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWm7MpRdkk0x",
        "outputId": "f894ad4c-7dda-493b-ca45-045a5428e28d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  44th/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frequency distribution\n",
        "\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to calculate frequency distribution\n",
        "def frequency_distribution(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    fd = FreqDist(tokens)\n",
        "    return fd\n",
        "\n",
        "# Test the function\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
        "fd = frequency_distribution(text)\n",
        "\n",
        "# Display the frequency distribution\n",
        "print(fd)\n",
        "print(\"Frequency of 'everyone':\", fd['everyone'])\n",
        "print(\"Total unique words:\", len(fd.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK_kU7Rdkxhy",
        "outputId": "23de1b31-93f7-4edf-fa4b-154775a8a860"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 11 samples and 13 outcomes>\n",
            "Frequency of 'everyone': 1\n",
            "Total unique words: 11\n"
          ]
        }
      ]
    }
  ]
}